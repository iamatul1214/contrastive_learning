{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras import metrics as M\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(emb_cols, emb_shape, num_shape):\n",
    "    embed_out_dim = 1\n",
    "\n",
    "    input_layers = []\n",
    "    embeddings = []\n",
    "    for emb_col in emb_cols:\n",
    "        emb_size = emb_shape[emb_col]\n",
    "        \n",
    "        # Define Embedding Input Layer\n",
    "        emb_input = Input(shape=(1,), name=f\"{emb_col}_emb\", dtype=tf.int32)\n",
    "        \n",
    "        # Define Embedding\n",
    "        emb = L.Embedding(emb_size, embed_out_dim, name=emb_col)(emb_input)\n",
    "        emb = L.Flatten()(emb)\n",
    "        \n",
    "        # Store in Dict\n",
    "        input_layers.append(emb_input)\n",
    "        embeddings.append(emb)\n",
    "    \n",
    "    # Numerical Feats Input\n",
    "    num_inp = Input(shape=(num_shape,), name=\"num_feats\", dtype=tf.float32)\n",
    "    #bn = L.BatchNormalization( name='Normalize')(num_inp)\n",
    "    num_emb = L.Dense(30, activation=\"relu\", name=\"num_emb\")(num_inp)\n",
    "    \n",
    "    base_embedding = tf.concat(embeddings + [num_emb], axis=1)\n",
    "    dense1 = L.Dense(30, activation=\"relu\", name=\"l1\")(base_embedding)\n",
    "    # dropout = L.Dropout(0.2)(dense1)\n",
    "    dense2 = L.Dense(16, activation=\"relu\", name=\"l2\")(dense1)\n",
    "    output = L.Dense(1, activation=\"sigmoid\", name=\"output\")(dense2)\n",
    "    \n",
    "    # Define Model\n",
    "    model = Model(input_layers+[num_inp], output)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam',\n",
    "                  metrics=[\n",
    "                      M.AUC(name='auc'),\n",
    "                      M.Precision(name='precision'),\n",
    "                      M.Recall(name='recall')\n",
    "                  ])\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "num_col_size = len(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = create_model(categorical_columns, embedding_size_dict, num_col_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_history = model.fit(\n",
    "    train_X, \n",
    "    train_df.is_fraud, \n",
    "    epochs=15, \n",
    "    batch_size=8192,\n",
    "    verbose=1,\n",
    "    validation_data=(test_X, test_df.is_fraud),\n",
    "    class_weight = class_weights,\n",
    "    #callbacks = [log_weights],\n",
    "    #sample_weight = train_df.txns/train_df.txns.max()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
