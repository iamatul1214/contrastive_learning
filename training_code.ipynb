{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from itertools import chain\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model, \n",
    "    inputs,\n",
    "    labels,\n",
    "    val_inputs,\n",
    "    val_labels,\n",
    "    test_inputs=None,\n",
    "    test_labels=None,\n",
    "    weight_decay=0.004, \n",
    "    label_smoothing=False, \n",
    "    recall_threshold=0.8, \n",
    "    save_path='.', \n",
    "    batch_size=512,\n",
    "    num_epochs=10,\n",
    "    learning_rate=1e-3, \n",
    "    fit_model=True\n",
    "):\n",
    "\n",
    "    # lr_schedule = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    #     boundaries=[200, 500, 1000,],\n",
    "    #     values=[learning_rate, learning_rate * 0.1, learning_rate * 0.01, learning_rate * 0.001]\n",
    "    # )\n",
    "    \n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=False, \n",
    "                                             name=\"binary_crossentropy\", label_smoothing=label_smoothing),\n",
    "        metrics=[\n",
    "            keras.metrics.AUC(from_logits=False, name='auc_roc'),\n",
    "            keras.metrics.PrecisionAtRecall(recall_threshold, num_thresholds=200, name='p_at_r'),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "    if fit_model:\n",
    "        model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"{save_path}/{datetime.datetime.now().strftime('%d%m_%H%M')}\" + '/' + \"{epoch:02d}-{val_loss:.3f}-{val_auc_roc:.3f}.tf\",\n",
    "            save_weights_only=False,\n",
    "            monitor='val_auc_roc',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "\n",
    "        )\n",
    "        \n",
    "        reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.5, patience=5\n",
    "        )\n",
    "        \n",
    "        early_stopping = keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            x=inputs,\n",
    "            y=labels,\n",
    "            batch_size=batch_size,\n",
    "            epochs=num_epochs,\n",
    "            # validation_split=0.15,\n",
    "            validation_data=(val_inputs, val_labels),\n",
    "            callbacks=[model_checkpoint, early_stopping, reduce_lr],\n",
    "            validation_freq=1,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        if test_inputs is not None:\n",
    "            _, auc_roc, p_at_r = model.evaluate(test_inputs, test_labels)\n",
    "            print(f\"Test auc_roc: {round(auc_roc * 100, 3)}%\")\n",
    "            print(f\"Test p_at_r: {round(p_at_r * 100, 3)}%\")\n",
    "        \n",
    "        return history\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_dense_layer_block(\n",
    "    name_tag, \n",
    "    output_dim, \n",
    "    add_dropout=True, \n",
    "    add_batch_norm=True, \n",
    "    activation_fn='relu',\n",
    "    dropout_prob=0.25\n",
    "):\n",
    "    dense_model = keras.Sequential(\n",
    "        keras.layers.Dense(output_dim, activation=None, name=f\"{name_tag}_dense\")\n",
    "    )\n",
    "    if activation_fn is not None:\n",
    "        keras_layers = importlib.import_module('tensorflow.keras.layers')\n",
    "        activation_layer = getattr(keras_layers, activation_fn)\n",
    "        dense_model.add(activation_layer())   \n",
    "    if add_batch_norm:\n",
    "        dense_model.add(\n",
    "            keras.layers.BatchNormalization(name=f\"{name_tag}_batch_norm\")\n",
    "        )\n",
    "    if add_dropout:\n",
    "        dense_model.add(\n",
    "            keras.layers.Dropout(rate=dropout_prob, name=f\"{name_tag}_dropout\")\n",
    "        )\n",
    "    return dense_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LoanStateModel(keras.Model):\n",
    "    def __init__(self, num_dense_blocks=1, emb_dim=30, activation_fn='relu'):\n",
    "        super().__init__()\n",
    "        self.dense_blocks = keras.Sequential([get_dense_layer_block(f\"loan_state_{i}\", emb_dim, activation_fn=activation_fn) \n",
    "                             for i in range(1, num_dense_blocks + 1)])\n",
    "        self.final_dense_block = get_dense_layer_block(\n",
    "            \"loan_state_final\",\n",
    "            emb_dim,\n",
    "            activation_fn=None,\n",
    "            add_batch_norm=False,\n",
    "            add_dropout=False\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.dense_blocks(x)\n",
    "        x = self.final_dense_block(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ActionModel(keras.Model):\n",
    "    \"\"\"\n",
    "    This architecture use seperate emb for each action. if there are 6 actions and 4 time slots, there are 24 actions\n",
    "    Hence, there will be 24 embeddings produced.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_actions, emb_dim=30, seq_len=6, num_dense_blocks=1, activation_fn='relu'):\n",
    "        super().__init__()\n",
    "        self.emb_layer = keras.layers.Embedding(input_dim=num_actions + 1, \n",
    "                                                output_dim=emb_dim, \n",
    "                                                mask_zero=True,\n",
    "                                                input_length=seq_len)\n",
    "        \n",
    "        self.dense_blocks = keras.Sequential([get_dense_layer_block(f\"action_{i}\", emb_dim, activation_fn=activation_fn) \n",
    "                             for i in range(1, num_dense_blocks + 1)])\n",
    "        self.final_dense_block = get_dense_layer_block(\n",
    "            \"action_final\",\n",
    "            emb_dim,\n",
    "            activation_fn=None,\n",
    "            add_batch_norm=False,\n",
    "            add_dropout=False\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.emb_layer(x)\n",
    "        x = tf.math.reduce_mean(x, axis=1, name='action_mean_pool')\n",
    "        x = self.dense_blocks(x)\n",
    "        x = self.final_dense_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class ActionModelSepEmb(keras.Model):\n",
    "    \"\"\"\n",
    "    This architecture use seperate emb for ch and time and then concatenates to get action embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels=6, num_time_slots=5, emb_dim=30, seq_len=6, num_dense_blocks=1, activation_fn='relu'):\n",
    "        super().__init__()\n",
    "        self.channel_emb_layer = keras.layers.Embedding(input_dim=num_channels + 1, \n",
    "                                                output_dim=emb_dim, \n",
    "                                                mask_zero=True,\n",
    "                                                input_length=seq_len)\n",
    "        \n",
    "        self.time_emb_layer = keras.layers.Embedding(input_dim=num_time_slots + 1, \n",
    "                                                output_dim=emb_dim, \n",
    "                                                mask_zero=True,\n",
    "                                                input_length=seq_len)\n",
    "        self.concat_layer = keras.layers.Concatenate(axis=-1)\n",
    "        self.dense_blocks = keras.Sequential([get_dense_layer_block(f\"action_{i}\", emb_dim, activation_fn=activation_fn) \n",
    "                             for i in range(1, num_dense_blocks + 1)])\n",
    "        self.final_dense_block = get_dense_layer_block(\n",
    "            \"action_final\",\n",
    "            emb_dim,\n",
    "            activation_fn=None,\n",
    "            add_batch_norm=False,\n",
    "            add_dropout=False\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        ch_inputs, time_inputs = inputs\n",
    "        ch_emb = self.channel_emb_layer(ch_inputs) \n",
    "        time_emb = self.time_emb_layer(time_inputs)\n",
    "        ch_time_concat = self.concat_layer([ch_emb, time_emb])\n",
    "        x = tf.math.reduce_mean(ch_time_concat, axis=1, name='action_mean_pool')\n",
    "        x = self.dense_blocks(x)\n",
    "        x = self.final_dense_block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LoanStateActionModel(keras.Model):\n",
    "    def __init__(self, loan_state_model_config, action_model_config, sep_ch_time_emb=True):\n",
    "        super().__init__()\n",
    "        self.loan_state_model = LoanStateModel(**loan_state_model_config)\n",
    "        if sep_ch_time_emb:\n",
    "            self.action_model = ActionModelSepEmb(**action_model_config)\n",
    "        else:\n",
    "            self.action_model = ActionModel(**action_model_config)\n",
    "        self.cosine_layer = keras.layers.Dot(axes=-1, normalize=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x1, x2 = inputs\n",
    "        loan_state_output = self.loan_state_model(x1)\n",
    "        action_output = self.action_model(x2)\n",
    "        x = self.cosine_layer([loan_state_output, action_output])\n",
    "        x = (x + 1) / 2\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class LoanStateActionContrastiveModel(keras.Model):\n",
    "    \"\"\"\n",
    "    Use contrastive learning on only positive samples\n",
    "    \"\"\"\n",
    "    def __init__(self, loan_state_model_config, action_model_config, sep_ch_time_emb=True, scale=5):\n",
    "        super().__init__()\n",
    "        self.scale = scale\n",
    "        self.loan_state_model = LoanStateModel(**loan_state_model_config)\n",
    "        if sep_ch_time_emb:\n",
    "            self.action_model = ActionModelSepEmb(**action_model_config)\n",
    "        else:\n",
    "            self.action_model = ActionModel(**action_model_config)\n",
    "        self.cosine_layer = keras.layers.Dot(axes=-1, normalize=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x1, x2 = inputs\n",
    "        loan_state_output = self.loan_state_model(x1)\n",
    "        action_output = self.action_model(x2)\n",
    "        loan_state_output = tf.expand_dims(loan_state_output, axis=0)\n",
    "        action_output = tf.expand_dims(action_output, axis=0)\n",
    "        x = self.cosine_layer([loan_state_output, action_output])\n",
    "        x = tf.squeeze(x, axis=0) * self.scale\n",
    "        # to reduce false negatives\n",
    "        tp = tf.linalg.diag_part(x)\n",
    "        mask = tf.cast(tf.equal(x, tf.expand_dims(tp, axis=1)), dtype=tf.float32)\n",
    "        fill = tf.zeros(mask.shape[0])\n",
    "        mask = tf.linalg.set_diag(mask, fill)\n",
    "        x = x * (1 - mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "loan_state_action_model = LoanStateActionModel({'num_dense_blocks': 2, 'activation_fn': 'LeakyReLU'}, {'activation_fn': 'LeakyReLU'}, sep_ch_time_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hisotry = run_experiment(\n",
    "    model=loan_state_action_model,\n",
    "    inputs=(x_train_state, (x_train_channels, x_train_times)),\n",
    "    labels=y_train,\n",
    "    val_inputs=(x_val_state, (x_val_channels, x_val_times)),\n",
    "    val_labels=y_val,\n",
    "    test_inputs=(x_test_state, (x_test_channels, x_test_times)),\n",
    "    test_labels=y_test,\n",
    "    save_path='trained_models/',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
